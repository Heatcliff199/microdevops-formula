# vi: set ft=yaml:
#
# usage:
#
# - Add pkg.k8s.lxd_host pillar to host machine if running inside LXD and apply.
#   Check 1048576 in /sys/module/nf_conntrack/parameters/hashsize after apply.
#
# - Prepare nodes, node fo nginx proxy (optional if vendor cloud balancer is not used), host for command.
#
# - Ensure nodes are accessible from command host by ssh.
#   Ensure needed firewall ports open on cluster nodes between each other (or all traffic open inside cluster).
#
# - Ensure internet and nodes are accessible from inside docker on nodes (may require nat_managed_docker_eth0, nat_managed_docker_eth1 etc).
#
# - Docker will manipulate iptables on nodes, so add input deny rules from internet if node has internet ip.
#
# - Generate new private key for usage in this pillar:
#   ssh-keygen -N "" -t ed25519 -f ./cluster_name -C cluster_name
#
# - Prepare this pillar and assign it for all nodes and command host.
#
# - salt -L 'srv1.example.com,*.rancher.dev.example.com' saltutil.refresh_pillar
#
# - Install binaries, cluster, yaml etc (ensure state run on command host, nginx nodes (if needed) and nodes:
#   salt '*.rancher.dev.example.com' state.apply rancher.prereqs
#
# - rke up:
#   salt srv1.example.com state.apply rancher.rke_up
#
# - Install helm, rancher chart and acme.sh cert (requires working acme.sh using pkg pillar):
#   salt srv1.example.com state.apply rancher.install
#
# - Open rancher web admin.
#
# - Give password and url.
#
# - Wait for provisioning.
#
# - Take bearer_token from admin - API & Keys - add key and put it below.
#
# - Apply users:
#   salt srv1.example.com state.apply rancher.users
#
# docker versions:
# - 18.06.3+ - works inside xenial, bionic LXD containers, but apparmor shoud be disabled on the host
# - 5:19.03.12+ - works inside focal LXD containers

rancher:
  cluster_name: rancher.dev.example.com
  cluster_domain: rancher.dev.example.com
  cluster_ssh_private_key: |
    -----BEGIN OPENSSH PRIVATE KEY-----
    ...
    -----END OPENSSH PRIVATE KEY-----
  cluster_ssh_public_key: |
    ssh-ed25519 AAAA... dev
  cluster_cidr: 10.42.0.0/16
  cluster_ip_range: 10.43.0.0/16
  cluster_dns_server: 10.43.0.10
  node_max_pods: 110
  command_hosts:
    - srv1.example.com
  nginx_hosts:
    - web1.rancher.dev.example.com
    - web2.rancher.dev.example.com
    - web3.rancher.dev.example.com
  docker-ce_version: 5:19.03.13
  rke_version: v1.2.3
  helm_version: v3.4.1
  rancher_cli_version: v2.4.9
  cluster_yml: cluster_v1.19.4.yml
  ingress_node_selector:
    dedicated: general
  monitoring_node_selector:
    dedicated: monitoring
  nodes:
    - address: cp1.rancher.dev.example.com
      port: 22
      internal_address: 10.0.10.2
      user: root
      role:
        - controlplane
        - etcd
      labels: {}
      taints: []
    - address: cp2.rancher.dev.example.com
      port: 22
      internal_address: 10.0.10.3
      user: root
      role:
        - controlplane
        - etcd
      labels: {}
      taints: []
    - address: cp3.rancher.dev.example.com
      port: 22
      internal_address: 10.0.10.4
      user: root
      role:
        - controlplane
        - etcd
      labels: {}
      taints: []
    - address: worker1.rancher.dev.example.com
      port: 22
      internal_address: 10.0.10.5
      user: root
      role:
        - worker
      labels:
        dedicated: general
      taints: []
    - address: worker2.rancher.dev.example.com
      port: 22
      internal_address: 10.0.10.6
      user: root
      role:
        - worker
      labels:
        dedicated: general
      taints: []
    - address: worker3.rancher.dev.example.com
      port: 22
      internal_address: 10.0.10.7
      user: root
      role:
        - worker
      labels:
        dedicated: general
      taints: []
    - address: mon1.rancher.dev.example.com
      port: 22
      internal_address: 10.0.10.8
      user: root
      role:
        - worker
      labels:
        dedicated: monitoring
      taints:
        - key: dedicated
          value: monitoring
          effect: PreferNoSchedule
  bearer_token: token-xxxxx:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx # add after install (taken from web admin), used to manage users via api
  users:
    - username: user
      password: password
      name: Some User
      description: Some Description
      must_change_password: False
