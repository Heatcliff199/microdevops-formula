# vi: set ft=yaml:
#
# usage:
#
# - add pkg.k8s.lxd_host pillar to host machine if running inside LXD and apply
#   (check 1048576 in /sys/module/nf_conntrack/parameters/hashsize after apply)
#
# - prepare nodes, node fo nginx proxy (optional if vendor cloud balancer is not used), host for command
#
# - ensure nodes are accessible from command host by ssh, needed firewall ports open on cluster nodes
#   docker will manipulate iptables on nodes, so add input deny rules from internet if node has internet ip
#
# - generate new private key for usage in this pillar
#   ssh-keygen -N "" -t ed25519 -f ./cluster_name -C cluster_name
#
# - prepare this pillar and assign it for all nodes and command host
#
# - salt -L 'srv1.example.com,*.rancher.dev.example.com' saltutil.refresh_pillar
#
# - install binaries, cluster, yaml etc (ensure state run on command host, nginx nodes (if needed) and nodes
#   salt '*.rancher.dev.example.com' state.apply rancher.prereqs
#
# - rke up
#   salt srv1.example.com state.apply rancher.rke_up
#
# - install helm, rancher chart and acme.sh cert (requires working acme.sh using pkg pillar)
#   salt srv1.example.com state.apply rancher.install
#
# - open rancher web admin
#
# - give password and url
#
# - wait for provisioning
#
# - take bearer_token from admin - API & Keys - add key and put it below
#
# - apply users
#   salt srv1.example.com state.apply rancher.users
#
# last two steps are not required to do from each command host and should be done once
#
# docker versions:
# - 18.06.3+ - works inside xenial, bionic LXD containers, but apparmor shoud be disabled on the host
# - 5:19.03.12 - works inside focal LXD containers

rancher:
  cluster_name: rancher.dev.example.com
  cluster_domain: rancher.dev.example.com
  cluster_ssh_private_key: |
    -----BEGIN OPENSSH PRIVATE KEY-----
    ...
    -----END OPENSSH PRIVATE KEY-----
  cluster_ssh_public_key: |
    ssh-ed25519 AAAA... dev
  cluster_cidr: 10.42.0.0/16
  cluster_ip_range: 10.43.0.0/16
  cluster_dns_server: 10.43.0.10
  node_max_pods: 110
  command_hosts:
    - srv1.example.com
  nginx_hosts:
    - web1.rancher.dev.example.com
    - web2.rancher.dev.example.com
    - web3.rancher.dev.example.com
  docker-ce_version: 5:19.03.13
  rke_version: v1.2.3
  helm_version: v3.4.1
  rancher_cli_version: v2.4.9
  cluster_yml: cluster_v1.19.4.yml
  ingress_node_selector:
    dedicated: commonGroup
  nodes:
    - address: cp1.rancher.dev.example.com
      port: 22
      internal_address: 10.0.10.2
      user: root
      role: 
        - controlplane
        - etcd
      labels: {}
      taints: []
    - address: cp2.rancher.dev.example.com
      port: 22
      internal_address: 10.0.10.3
      user: root
      role: 
        - controlplane
        - etcd
      labels: {}
      taints: []
    - address: cp3.rancher.dev.example.com
      port: 22
      internal_address: 10.0.10.4
      user: root
      role: 
        - controlplane
        - etcd
      labels: {}
      taints: []
    - address: worker1.rancher.dev.example.com
      port: 22
      internal_address: 10.0.10.5
      user: root
      role: 
        - worker
      labels:
        dedicated: commonGroup
      taints:
        - key: dedicated
          value: commonGroup
          effect: NoSchedule
    - address: worker2.rancher.dev.example.com
      port: 22
      internal_address: 10.0.10.6
      user: root
      role: 
        - worker
      labels:
        dedicated: commonGroup
      taints:
        - key: dedicated
          value: commonGroup
          effect: NoSchedule
    - address: worker3.rancher.dev.example.com
      port: 22
      internal_address: 10.0.10.7
      user: root
      role: 
        - worker
      labels:
        dedicated: commonGroup
      taints:
        - key: dedicated
          value: commonGroup
          effect: NoSchedule
    - address: mon1.rancher.dev.example.com
      port: 22
      internal_address: 10.0.10.8
      user: root
      role: 
        - worker
      labels:
        dedicated: monitoringGroup
      taints:
        - key: dedicated
          value: monitoringGroup
          effect: NoSchedule
  bearer_token: token-xxxxx:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx # add after install (taken from web admin), used to manage users via api
  users:
    - username: user
      password: password
      name: Some User
      description: Some Description
      must_change_password: False
